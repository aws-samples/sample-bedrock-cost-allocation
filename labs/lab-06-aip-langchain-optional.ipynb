{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Lab 06: AIP with LangChain - Multi-Tenant Workflow Chains\n\n## Business Context\n\nYou are building a **Multi-Tenant Marketing Platform** using LangChain that serves multiple enterprise clients. Your platform generates AI-powered marketing content through multi-step chains:\n\n- **Tenant A**: B2B tech company needing campaign content workflows\n- **Tenant B**: B2C retail company needing product description workflows\n\n**Challenge**: Implement LangChain chains with Application Inference Profiles for per-tenant isolation and cost tracking.\n\n## Learning Objectives\n- Integrate Application Inference Profiles with LangChain\n- Create tenant-specific LLM instances using Bedrock\n- Build simple multi-step chains with AIP integration\n- Track token usage and costs per tenant\n- Verify per-tenant CloudWatch metrics isolation\n\n## What's Different from Previous Labs?\n- **Lab 03 (Boto3)**: Direct API calls, manual chain logic\n- **Lab 04 (Strands)**: Agent framework with automatic tool orchestration\n- **Lab 05 (LiteLLM)**: Gateway abstraction for routing\n- **Lab 06 (LangChain)**: Declarative chains with automatic prompt composition\n\n## Focus: Application Inference Profiles\nThis lab keeps complexity low and focuses on **AIP integration**, not advanced LangChain features."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --force-reinstall -q -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Create Application Inference Profiles\n",
    "\n",
    "First, let's set up our environment with boto3 (for AIP management) and LangChain (for chain workflows), then check for existing AIPs or create new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# Import lab helpers\n",
    "from lab_helpers.config import Region, ModelId\n",
    "from lab_helpers.aip_manager import AIPManager\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_client = boto3.client('bedrock', region_name=Region)\n",
    "sts = boto3.client('sts', region_name=Region)\n",
    "\n",
    "print(f\"âœ… Initialized boto3 clients for region: {Region}\")\n",
    "print(f\"âœ… Initialized LangChain for chain workflows\")\n",
    "print(f\"ğŸ“‹ Using Claude Sonnet 4 model: {ModelId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ—ï¸ Setup: AIP Manager and Tenant Configurations\n",
    "\n",
    "Initialize the Application Inference Profile manager and define configurations for two tenants:\n",
    "- **Tenant A (B2B Tech)**: SaaS marketing content generation\n",
    "- **Tenant B (B2C Retail)**: Product description and marketing copy\n",
    "\n",
    "Each tenant gets:\n",
    "- Unique AIP for isolated model access\n",
    "- Tags for cost tracking and billing\n",
    "- Separate CloudWatch metrics dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AIP Manager\n",
    "aip_manager = AIPManager(bedrock_client)\n",
    "\n",
    "# Define tenant configurations (same as previous labs)\n",
    "TENANT_CONFIGS = {\n",
    "    \"tenant_a\": {\n",
    "        \"name\": \"marketing-ai-tenant-a\",\n",
    "        \"description\": \"Marketing AI AIP for Tenant A\",\n",
    "        \"tags\": {\n",
    "            \"TenantId\": \"tenant-a\",\n",
    "            \"BusinessType\": \"B2B-Tech\",\n",
    "            \"Environment\": \"production\",\n",
    "            \"CostCenter\": \"marketing-ai-platform\"\n",
    "        }\n",
    "    },\n",
    "    \"tenant_b\": {\n",
    "        \"name\": \"marketing-ai-tenant-b\", \n",
    "        \"description\": \"Marketing AI AIP for Tenant B\",\n",
    "        \"tags\": {\n",
    "            \"TenantId\": \"tenant-b\",\n",
    "            \"BusinessType\": \"B2C-Retail\",\n",
    "            \"Environment\": \"production\",\n",
    "            \"CostCenter\": \"marketing-ai-platform\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ğŸ“‹ Tenant configurations defined:\")\n",
    "for tenant_id, config in TENANT_CONFIGS.items():\n",
    "    print(f\"  - {tenant_id}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ” Check for Existing AIPs (Reuse from Previous Labs)\n",
    "\n",
    "Before creating new Application Inference Profiles, let's check if they already exist from Lab 03-05.\n",
    "\n",
    "**Strategy:**\n",
    "1. Check if AIPs exist with the same names\n",
    "2. If exists: **Reuse them** (saves time and avoids duplicates)\n",
    "3. If not exists: **Create new ones** using the same logic as Lab 03\n",
    "\n",
    "This approach ensures we can run Lab 06 independently or after previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing AIPs and reuse or create\n",
    "tenant_aips = {}\n",
    "\n",
    "for tenant_id, config in TENANT_CONFIGS.items():\n",
    "    print(f\"\\nğŸ” Checking AIP for {tenant_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if AIP already exists (possibly from Lab 03-05)\n",
    "        existing_arn = aip_manager.check_aip_exists(config[\"name\"])\n",
    "        \n",
    "        if existing_arn:\n",
    "            print(f\"âœ… Found existing AIP (reusing from previous lab)\")\n",
    "            print(f\"   ARN: {existing_arn}\")\n",
    "            tenant_aips[tenant_id] = existing_arn\n",
    "        else:\n",
    "            print(f\"ğŸ“ AIP not found - creating new one...\")\n",
    "            \n",
    "            # Prepare tags\n",
    "            tag_list = []\n",
    "            if config[\"tags\"]:\n",
    "                tag_list = [{\"key\": k, \"value\": v} for k, v in config[\"tags\"].items()]\n",
    "            \n",
    "            # Get account ID\n",
    "            account_id = sts.get_caller_identity()['Account']\n",
    "            \n",
    "            # Construct proper ARN\n",
    "            MODEL_ARN = f\"arn:aws:bedrock:{Region}:{account_id}:inference-profile/{ModelId}\"\n",
    "            \n",
    "            # Create Application Inference Profile\n",
    "            response = bedrock_client.create_inference_profile(\n",
    "                inferenceProfileName=config[\"name\"],\n",
    "                description=config[\"description\"],\n",
    "                modelSource={\"copyFrom\": MODEL_ARN},\n",
    "                tags=tag_list\n",
    "            )\n",
    "            \n",
    "            aip_arn = response['inferenceProfileArn']\n",
    "            tenant_aips[tenant_id] = aip_arn\n",
    "            \n",
    "            print(f\"âœ… Created new AIP for {tenant_id}\")\n",
    "            print(f\"   Status: {response['status']}\")\n",
    "            print(f\"   ARN: {aip_arn}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error with AIP for {tenant_id}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary: {len(tenant_aips)} Application Inference Profiles ready for LangChain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Create Tenant-Specific LangChain LLM Instances\n",
    "\n",
    "Now let's create LangChain ChatBedrock instances for each tenant, configured with their specific Application Inference Profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tenant-specific LangChain LLM instances\n",
    "tenant_llms = {}\n",
    "\n",
    "for tenant_id, aip_arn in tenant_aips.items():\n",
    "    print(f\"\\nğŸ¤– Creating LangChain LLM for {tenant_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Create ChatBedrock instance with tenant's AIP\n",
    "        # This is the key pattern: use aip_arn instead of system model ID\n",
    "        llm = ChatBedrock(\n",
    "            model_id=aip_arn,  # â† Use AIP ARN for tenant isolation\n",
    "            region_name=Region,\n",
    "            model_kwargs={\n",
    "                \"temperature\": 0.7,\n",
    "                \"max_tokens\": 1000\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        tenant_llms[tenant_id] = llm\n",
    "        \n",
    "        print(f\"âœ… Created LangChain LLM for {tenant_id}\")\n",
    "        print(f\"   Model: ChatBedrock with AIP\")\n",
    "        print(f\"   AIP ARN: {aip_arn}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error creating LLM for {tenant_id}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Summary: {len(tenant_llms)} tenant-specific LangChain LLMs ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š CloudWatch Monitoring Function\n",
    "\n",
    "Same monitoring function as previous labs - works with all approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_aip_usage(tenant_aips, region):\n",
    "    \"\"\"\n",
    "    Monitor CloudWatch metrics for Application Inference Profiles.\n",
    "    Works with all approaches (Boto3, Strands, LiteLLM, LangChain, LangGraph).\n",
    "    \"\"\"\n",
    "    from lab_helpers.cloudwatch import fetch_metrices, plot_graph\n",
    "\n",
    "    print(\"ğŸ“Š Fetching CloudWatch metrics for Application Inference Profiles...\")\n",
    "    print(f\"Region: {region}\")\n",
    "    print(f\"Time Range: Last 60 minutes\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    tenant_metrics = {}\n",
    "\n",
    "    for tenant_id, aip_arn in tenant_aips.items():\n",
    "        print(f\"\\nğŸ¢ TENANT: {tenant_id.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"AIP ARN: {aip_arn}\")\n",
    "        \n",
    "        # Extract AIP ID from full ARN for CloudWatch ModelId dimension\n",
    "        aip_id = aip_arn.split('/')[-1]\n",
    "        print(f\"AIP ID: {aip_id}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nğŸ“Š METRICS FOR {tenant_id.upper()}:\")\n",
    "            response, input_token_response, output_token_response = fetch_metrices(\n",
    "                Region=region,\n",
    "                Period=60,\n",
    "                Timedelta=60,\n",
    "                Id=aip_id\n",
    "            )\n",
    "            \n",
    "            tenant_metrics[tenant_id] = {\n",
    "                'invocations': response,\n",
    "                'input_tokens': input_token_response,\n",
    "                'output_tokens': output_token_response\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nğŸ“ˆ USAGE PLOTS FOR {tenant_id.upper()}:\")\n",
    "            plot_graph(response, input_token_response, output_token_response)\n",
    "            \n",
    "            print(\"=\"*50)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ CloudWatch error for {tenant_id}: {str(e)}\")\n",
    "            print(f\"ğŸ’¡ Metrics may take a few minutes to appear after chain invocations\")\n",
    "            print(\"=\"*50)\n",
    "    \n",
    "    return tenant_metrics\n",
    "\n",
    "print(\"âœ… CloudWatch monitoring function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”µ BEFORE Check: Baseline CloudWatch Metrics\n",
    "\n",
    "Check CloudWatch metrics before running any chains to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CloudWatch metrics BEFORE any chain invocations\n",
    "print(\"ğŸ”µ BEFORE CHECK: Querying CloudWatch for baseline metrics...\")\n",
    "print(\"=\"*80)\n",
    "print(\"Expected: Empty if this is your first LangChain run\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "baseline_metrics = monitor_aip_usage(tenant_aips, Region)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… Baseline check complete!\")\n",
    "print(\"ğŸ’¡ Note: Empty metrics are expected if this is your first run\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Define Simple Content Generation Chains\n",
    "\n",
    "Create simple, focused chains for each tenant. We keep this intentionally simple to focus on **AIP integration**, not complex LangChain features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Define simple chains for each tenant\n",
    "# Chain 1: Campaign Headline Generator (Tenant A - B2B)\n",
    "headline_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\", \"audience\"],\n",
    "    template=\"\"\"Create a compelling marketing headline for a {product} targeting {audience}.\n",
    "    The headline should be attention-grabbing and convey clear value.\n",
    "    Respond with just the headline, nothing else.\"\"\"\n",
    ")\n",
    "\n",
    "# Chain 2: Product Description Generator (Tenant B - B2C)\n",
    "description_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\", \"features\"],\n",
    "    template=\"\"\"Write a compelling product description for {product}.\n",
    "    Key features: {features}\n",
    "    Keep it under 100 words, engaging and customer-focused.\n",
    "    Respond with just the description, nothing else.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Content generation prompts defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¯ Create Tenant-Specific Chains\n",
    "\n",
    "Build simple chains that route through each tenant's AIP-backed LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chains for each tenant using their AIP-backed LLM\n",
    "tenant_chains = {}\n",
    "\n",
    "print(\"\\nğŸ”— Creating tenant-specific chains...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Tenant A chain: Headline generation\n",
    "if \"tenant_a\" in tenant_llms:\n",
    "    tenant_a_chain = headline_prompt | tenant_llms[\"tenant_a\"] | StrOutputParser()\n",
    "    tenant_chains[\"tenant_a\"] = {\n",
    "        \"chain\": tenant_a_chain,\n",
    "        \"type\": \"headline_generator\",\n",
    "        \"input_vars\": {\"product\": \"DevOps automation platform\", \"audience\": \"CTOs at enterprise companies\"}\n",
    "    }\n",
    "    print(\"\\nâœ… Tenant A Chain: Headline Generator\")\n",
    "    print(f\"   LLM: ChatBedrock with AIP\")\n",
    "    print(f\"   Purpose: Generate marketing headlines for B2B tech products\")\n",
    "\n",
    "# Tenant B chain: Product description generation\n",
    "if \"tenant_b\" in tenant_llms:\n",
    "    tenant_b_chain = description_prompt | tenant_llms[\"tenant_b\"] | StrOutputParser()\n",
    "    tenant_chains[\"tenant_b\"] = {\n",
    "        \"chain\": tenant_b_chain,\n",
    "        \"type\": \"description_generator\",\n",
    "        \"input_vars\": {\"product\": \"Sustainable Summer Fashion Collection\", \"features\": \"eco-friendly materials, hand-crafted, limited edition\"}\n",
    "    }\n",
    "    print(\"\\nâœ… Tenant B Chain: Product Description Generator\")\n",
    "    print(f\"   LLM: ChatBedrock with AIP\")\n",
    "    print(f\"   Purpose: Generate product descriptions for B2C retail products\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(f\"ğŸ“Š Created {len(tenant_chains)} tenant-specific chains\")\nprint(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ Execute Chains for Each Tenant\n",
    "\n",
    "Run the chains and capture outputs with per-tenant tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute chains for each tenant\n",
    "chain_results = {}\n",
    "\n",
    "print(\"\\nğŸš€ Executing tenant-specific chains...\")\nprint(\"=\"*80)\n",
    "\n",
    "for tenant_id, chain_config in tenant_chains.items():\n",
    "    print(f\"\\nğŸ¢ Processing {tenant_id.upper()}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Record start time for latency calculation\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Execute the chain with tenant-specific inputs\n",
    "        output = chain_config[\"chain\"].invoke(chain_config[\"input_vars\"])\n",
    "        \n",
    "        # Calculate latency\n",
    "        end_time = datetime.now()\n",
    "        latency_ms = (end_time - start_time).total_seconds() * 1000\n",
    "        \n",
    "        # Store result\n",
    "        chain_results[tenant_id] = {\n",
    "            \"type\": chain_config[\"type\"],\n",
    "            \"output\": output,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"success\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"âœ… Chain executed for {tenant_id}\")\n",
    "        print(f\"   Output: {output[:80]}...\")\n",
    "        print(f\"   Latency: {latency_ms:.2f}ms\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error executing chain for {tenant_id}: {str(e)}\")\n",
    "        chain_results[tenant_id] = {\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\nprint(f\"ğŸ“Š Executed {len([r for r in chain_results.values() if r['success']])} successful chains\")\nprint(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Review Results and CloudWatch Metrics\n",
    "\n",
    "Display the generated outputs and verify per-tenant CloudWatch metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“„ Display Chain Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display chain outputs\n",
    "print(\"ğŸ“„ CHAIN OUTPUTS\")\nprint(\"=\"*80)\n",
    "\n",
    "for tenant_id, result in chain_results.items():\n",
    "    if result['success']:\n",
    "        print(f\"\\nğŸ¢ {tenant_id.upper()} - {result['type']}\")\n",
    "        print(\"-\"*50)\n",
    "        print(f\"Output:\\n{result['output']}\")\n",
    "        print(f\"\\nLatency: {result['latency_ms']:.2f}ms\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(f\"\\nâŒ {tenant_id.upper()}: {result['error']}\")\n",
    "        print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŸ¢ AFTER Check: CloudWatch Metrics Show Multi-Tenant Isolation\n",
    "\n",
    "Now let's check CloudWatch metrics after chain execution to verify per-tenant isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for CloudWatch metrics to propagate\nprint(\"â³ Waiting 60 seconds for CloudWatch metrics to propagate...\")\ntime.sleep(60)\nprint(\"âœ… Wait complete - proceeding with metrics monitoring\\n\")\n",
    "\n",
    "# Check CloudWatch metrics AFTER chain execution\nprint(\"ğŸŸ¢ AFTER CHECK: Querying CloudWatch for post-chain metrics...\")\nprint(\"=\"*80)\nprint(\"Expected: Per-tenant metrics showing isolated usage!\")\nprint(\"=\"*80 + \"\\n\")\n",
    "\n",
    "after_metrics = monitor_aip_usage(tenant_aips, Region)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\nprint(\"âœ… Metrics check complete!\")\nprint(\"ğŸ’¡ If you see metrics data, multi-tenant isolation is working!\")\nprint(\"   Each tenant's chain execution tracked separately via their AIP.\")\nprint(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Before/After Comparison\n",
    "\n",
    "Summary of the multi-tenant isolation proof through LangChain + AIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\nâ•‘              LANGCHAIN + AIP: Multi-Tenant Isolation Proof                   â•‘\nâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nğŸ”µ BEFORE (Baseline Check):\n   Status: AIPs created and configured for LangChain\n   Chains: Ready but unused\n   CloudWatch: No metrics (AIPs unused)\n   Tenant Isolation: Ready but unverified\n\nğŸŸ¢ AFTER (Post-Chain Execution):\n   Chains: âœ… Executed via tenant-specific AIP-backed LLMs\n   Outputs: âœ… Generated per-tenant content\n   CloudWatch Metrics: âœ… Separate dimensions per tenant\n   Cost Tracking: âœ… Per-tenant billing enabled\n\nğŸ“Š WHAT WE PROVED:\n   1. âœ… LangChain ChatBedrock integrates with AIPs\n   2. âœ… Each tenant's chain execution tracked separately in CloudWatch\n   3. âœ… Simple prompt â†’ LLM â†’ output pattern scales to N tenants\n   4. âœ… AIP provides transparent per-tenant isolation in LangChain\n   5. âœ… Accurate cost allocation with LangChain chains\n\nğŸ’¡ THE PATTERN:\n   ChatBedrock(model_id=aip_arn)  â†’  Prompt | LLM | Parser  â†’  CloudWatch\n   (Tenant-specific)                (Chain execution)            (Isolated metrics)\n\nâœ¨ KEY INSIGHT:\n   LangChain chains + AIPs = Production-ready multi-tenant content generation\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Summary\n",
    "\n",
    "ğŸ‰ **Congratulations!** You've successfully integrated Application Inference Profiles with LangChain chains for a multi-tenant content generation platform.\n",
    "\n",
    "### What You Accomplished:\n",
    "\n",
    "1. **âœ… Checked and reused existing AIPs** from previous labs\n",
    "2. **âœ… Created tenant-specific ChatBedrock LLM instances** using AIP ARNs\n",
    "3. **âœ… Built simple LangChain prompt chains** with AIP integration\n",
    "4. **âœ… Checked CloudWatch metrics BEFORE** chain execution (baseline/empty state)\n",
    "5. **âœ… Executed tenant-specific chains** with isolated routing\n",
    "6. **âœ… Checked CloudWatch metrics AFTER** chain execution (per-tenant usage visible!)\n",
    "7. **âœ… Proved multi-tenant isolation** works with LangChain\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **AIP Integration**: ChatBedrock + aip_arn enables automatic per-tenant tracking\n",
    "- **Chain Simplicity**: Prompt | LLM | Parser pattern is clean and composable\n",
    "- **Tenant Isolation**: CloudWatch metrics remain separate despite LangChain abstraction\n",
    "- **Cost Tracking**: Built-in token usage enables accurate billing\n",
    "- **Multi-Step Workflows**: Can compose multiple chains for complex workflows\n",
    "\n",
    "### The Pattern:\n",
    "\n",
    "```python\n",
    "# For each tenant:\n",
    "llm = ChatBedrock(model_id=aip_arn)  # AIP provides isolation\n",
    "chain = prompt | llm | parser        # Simple declarative chain\n",
    "output = chain.invoke(inputs)        # Automatic CloudWatch tracking\n",
    "```\n",
    "\n",
    "### Framework Comparison:\n",
    "\n",
    "| Lab | Framework | Code Simplicity | Flexibility | Best For |\n",
    "|-----|-----------|-----------------|-------------|----------|\n",
    "| Lab 03 | Boto3 | Low | High | Fine-grained control |\n",
    "| Lab 04 | Strands | Medium | Medium | Agents with tools |\n",
    "| Lab 05 | LiteLLM | High | Low | Gateway patterns |\n",
    "| **Lab 06** | **LangChain** | **High** | **Medium** | **Simple chains & composition** |\n",
    "| Lab 07 | LangGraph | High | High | Complex workflows |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Continue your AIP journey with graph-based workflows:\n",
    "- **Lab 07**: LangGraph - Graph workflows for complex multi-step processes\n",
    "\n",
    "**Ready for complex workflows?** â†’ [Continue to Lab 07: LangGraph](lab-07-aip-langgraph-optional.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}