{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05: AIP with LiteLLM - AI Gateway for Multi-Tenant Applications\n",
    "\n",
    "## Business Context\n",
    "\n",
    "You are building a **Multi-Tenant Marketing Platform** with an API gateway layer that serves multiple enterprise clients with AI-powered capabilities. Your platform provides a unified LiteLLM gateway that routes requests to Nova Pro for different tenants:\n",
    "\n",
    "- **Tenant A**: B2B tech company needing marketing campaign automation\n",
    "- **Tenant B**: B2C retail company needing promotional content generation\n",
    "\n",
    "**Challenge**: Use LiteLLM as an abstraction layer while maintaining per-tenant isolation through Application Inference Profiles, enabling simpler multi-tenant code and configuration-driven tenant management.\n",
    "\n",
    "## Learning Objectives\n",
    "- Configure LiteLLM to route requests through Application Inference Profiles\n",
    "- Create tenant-specific gateway configurations for multi-tenant isolation\n",
    "- Route requests through LiteLLM to tenant-specific AIPs\n",
    "- Track usage and costs per tenant through AIP metrics\n",
    "- Compare gateway abstraction (Lab 05) vs direct SDK (Lab 03) approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --force-reinstall -q -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Setup and Create Application Inference Profiles\n",
    "\n",
    "First, let's set up our environment with both boto3 (for AIP management) and LiteLLM (for gateway abstraction), then create Application Inference Profiles for our two tenants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import litellm\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "from lab_helpers.config import Region, ModelId\n",
    "from lab_helpers.aip_manager import AIPManager\n",
    "from lab_helpers.usage_tracker import UsageTracker\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_client = boto3.client('bedrock', region_name=Region)\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=Region)\n",
    "sts = boto3.client('sts', region_name=Region)\n",
    "\n",
    "# Configure LiteLLM to suppress verbose logs\n",
    "litellm.set_verbose = False\n",
    "\n",
    "print(f\"‚úÖ Initialized boto3 clients for region: {Region}\")\n",
    "print(f\"‚úÖ Initialized LiteLLM for gateway abstraction\")\n",
    "print(f\"üìã Using Nova Pro 1.0 model: {ModelId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Setup: AIP Manager and Tenant Configurations\n",
    "\n",
    "Initialize the Application Inference Profile manager and define configurations for two tenants:\n",
    "- **Tenant A (SaaS Support)**: Customer support automation platform.\n",
    "- **Tenant B (Analytics)**: Report generation and data analysis service.\n",
    "\n",
    "Each tenant gets:\n",
    "- Unique AIP for isolated model access\n",
    "- Tags for cost tracking and billing (`TenantId`, `BusinessType`, `CostCenter`)\n",
    "- Separate CloudWatch metrics dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize AIP Manager\n",
    "aip_manager = AIPManager(bedrock_client)\n",
    "\n",
    "# Define tenant configurations (same as Lab 03)\n",
    "TENANT_CONFIGS = {\n",
    "    \"tenant_a\": {\n",
    "        \"name\": \"marketing-ai-tenant-a\",\n",
    "        \"description\": \"Marketing AI AIP for Tenant A\",\n",
    "        \"tags\": {\n",
    "            \"TenantId\": \"tenant-a\",\n",
    "            \"BusinessType\": \"B2B-Tech\",\n",
    "            \"Environment\": \"production\",\n",
    "            \"CostCenter\": \"marketing-ai-platform\"\n",
    "        }\n",
    "    },\n",
    "    \"tenant_b\": {\n",
    "        \"name\": \"marketing-ai-tenant-b\", \n",
    "        \"description\": \"Marketing AI AIP for Tenant B\",\n",
    "        \"tags\": {\n",
    "            \"TenantId\": \"tenant-b\",\n",
    "            \"BusinessType\": \"B2C-Retail\",\n",
    "            \"Environment\": \"production\",\n",
    "            \"CostCenter\": \"marketing-ai-platform\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Tenant configurations defined:\")\n",
    "for tenant_id, config in TENANT_CONFIGS.items():\n",
    "    print(f\"  - {tenant_id}: {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Create Application Inference Profiles for Each Tenant\n",
    "\n",
    "This cell creates (or verifies existing) Application Inference Profiles for both tenants using the AWS Bedrock API.\n",
    "\n",
    "**The AIP Creation Process:**\n",
    "\n",
    "1. **Check for Existing AIPs**: Verify if tenant already has an AIP to avoid duplicates\n",
    "2. **Prepare Tags**: Convert tenant tags to AWS API format for cost allocation\n",
    "3. **Construct Model ARN**: Build proper ARN pointing to the base System Inference Profile\n",
    "4. **Create AIP**: Call `create_inference_profile()` with:\n",
    "   - Unique profile name per tenant\n",
    "   - Model source (copies from System Inference Profile)\n",
    "   - Tenant-specific tags for tracking\n",
    "\n",
    "**üéØ The Critical API Call:**\n",
    "```python\n",
    "bedrock_client.create_inference_profile(\n",
    "    inferenceProfileName=config[\"name\"],\n",
    "    description=config[\"description\"],\n",
    "    modelSource={\"copyFrom\": MODEL_ARN},\n",
    "    tags=tag_list\n",
    ")\n",
    "```\n",
    "\n",
    "**What you get:**\n",
    "- Unique AIP ARN for each tenant\n",
    "- Isolated CloudWatch metrics dimension (ModelId = AIP ARN)\n",
    "- Tagged resources for cost allocation and billing\n",
    "\n",
    "**Result:** Two Application Inference Profiles ready for LiteLLM gateway routing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing AIPs and reuse or create\n",
    "tenant_aips = {}\n",
    "\n",
    "for tenant_id, config in TENANT_CONFIGS.items():\n",
    "    print(f\"\\nüîç Checking AIP for {tenant_id}...\")\n",
    "    \n",
    "    try:\n",
    "        # Check if AIP already exists (possibly from Lab 03)\n",
    "        existing_arn = aip_manager.check_aip_exists(config[\"name\"])\n",
    "        \n",
    "        if existing_arn:\n",
    "            print(f\"‚úÖ Found existing AIP (reusing from Lab 03 or previous run)\")\n",
    "            print(f\"   ARN: {existing_arn}\")\n",
    "            tenant_aips[tenant_id] = existing_arn\n",
    "        else:\n",
    "            print(f\"üìù AIP not found - creating new one...\")\n",
    "            \n",
    "            # Prepare tags\n",
    "            tag_list = []\n",
    "            if config[\"tags\"]:\n",
    "                tag_list = [{\"key\": k, \"value\": v} for k, v in config[\"tags\"].items()]\n",
    "            \n",
    "            # Get account ID\n",
    "            account_id = sts.get_caller_identity()['Account']\n",
    "            \n",
    "            # Construct proper ARN\n",
    "            MODEL_ARN = f\"arn:aws:bedrock:{Region}:{account_id}:inference-profile/{ModelId}\"\n",
    "            print(f\"   Model ARN: {MODEL_ARN}\")\n",
    "            \n",
    "            # Create Application Inference Profile\n",
    "            response = bedrock_client.create_inference_profile(\n",
    "                inferenceProfileName=config[\"name\"],\n",
    "                description=config[\"description\"],\n",
    "                modelSource={\"copyFrom\": MODEL_ARN},\n",
    "                tags=tag_list\n",
    "            )\n",
    "            \n",
    "            aip_arn = response['inferenceProfileArn']\n",
    "            tenant_aips[tenant_id] = aip_arn\n",
    "            \n",
    "            print(f\"‚úÖ Created new AIP for {tenant_id}\")\n",
    "            print(f\"   Status: {response['status']}\")\n",
    "            print(f\"   ARN: {aip_arn}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error with AIP for {tenant_id}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nüìä Summary: {len(tenant_aips)} Application Inference Profiles ready for Litellm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåâ Why LiteLLM? The Gateway Abstraction Layer\n",
    "\n",
    "**LiteLLM** provides a unified interface for calling 100+ LLMs through a consistent OpenAI-style API. When combined with Application Inference Profiles, it enables:\n",
    "\n",
    "**‚úÖ Simplified Multi-Tenant Code:**\n",
    "- Single `litellm.completion()` call for all tenants\n",
    "- Configuration-driven routing (not hardcoded logic)\n",
    "- Same code works across multiple models/providers\n",
    "\n",
    "**‚úÖ Gateway Features:**\n",
    "- Unified interface across AWS Bedrock, OpenAI, Azure, etc.\n",
    "- Built-in token counting and cost tracking\n",
    "- Automatic error handling and retries\n",
    "- Easy model switching via configuration\n",
    "\n",
    "**‚úÖ Production Benefits:**\n",
    "- Easier to scale to many tenants (just add config)\n",
    "- Simpler codebase (less conditional logic)\n",
    "- Model-agnostic application code\n",
    "- Gateway pattern ready for deployment\n",
    "\n",
    "**The Key Pattern:**\n",
    "\n",
    "```python\n",
    "# Lab 03 (Direct boto3): Different code paths, manual routing\n",
    "if tenant_id == \"tenant_a\":\n",
    "    response = bedrock_runtime.invoke_model(modelId=tenant_a_aip_arn, ...)\n",
    "elif tenant_id == \"tenant_b\":\n",
    "    response = bedrock_runtime.invoke_model(modelId=tenant_b_aip_arn, ...)\n",
    "\n",
    "# Lab 05 (LiteLLM): Unified interface, configuration-driven\n",
    "response = litellm.completion(\n",
    "    model=litellm_config[tenant_id][\"model\"],  # Config maps to AIP\n",
    "    messages=[{\"role\": \"user\", \"content\": message}]\n",
    ")\n",
    "```\n",
    "\n",
    "**Next:** Configure LiteLLM to route through tenant-specific AIPs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Configure LiteLLM Gateway for Multi-Tenant Routing\n",
    "\n",
    "Create configuration mappings that tell LiteLLM how to route each tenant's requests to their specific Application Inference Profile.\n",
    "\n",
    "**Configuration Structure:**\n",
    "- **model**: LiteLLM uses `bedrock/<model_id>` format to route to AWS Bedrock\n",
    "- **aws_region_name**: Specify AWS region for Bedrock calls\n",
    "- **max_tokens**: Control response length per tenant\n",
    "- **temperature**: Adjust creativity/determinism per tenant\n",
    "\n",
    "**Key Insight:** Each tenant's config points to their unique AIP ARN, enabling LiteLLM to transparently route through tenant-isolated AIPs while maintaining CloudWatch metric separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LiteLLM gateway mappings for each tenant\n",
    "litellm_config = {}\n",
    "\n",
    "for tenant_id, aip_arn in tenant_aips.items():\n",
    "    # LiteLLM requires full ARN with bedrock/converse/ prefix for AIPs\n",
    "    # Based on working example: model: bedrock/converse/arn:aws:bedrock:...\n",
    "    litellm_config[tenant_id] = {\n",
    "        \"model\": f\"bedrock/converse/{aip_arn}\",  # Full ARN format\n",
    "        \"aws_region_name\": Region,\n",
    "        \"max_tokens\": 1000,\n",
    "        # Note: temperature removed - not supported with Bedrock AIPs\n",
    "        \"metadata\": {\n",
    "            \"tenant_id\": tenant_id,\n",
    "            \"aip_arn\": aip_arn\n",
    "        }\n",
    "    }\n",
    "    \n",
    "print(\"üåâ LiteLLM Gateway Configuration:\")\n",
    "print(\"=\"*60)\n",
    "for tenant_id, config in litellm_config.items():\n",
    "    print(f\"\\nüè¢ {tenant_id.upper()}:\")\n",
    "    print(f\"   Model: {config['model']}\")\n",
    "    print(f\"   Region: {config['aws_region_name']}\")\n",
    "    print(f\"   Max Tokens: {config['max_tokens']}\")\n",
    "    print(f\"   AIP ARN: {config['metadata']['aip_arn']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ LiteLLM gateway ready to route tenant requests through AIPs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìä CloudWatch Monitoring Function\n",
    "\n",
    "The `monitor_aip_usage()` function fetches and visualizes CloudWatch metrics for each tenant's Application Inference Profile.\n",
    "\n",
    "**What it does:**\n",
    "- Queries CloudWatch for the last 60 minutes of data\n",
    "- Fetches three key metrics per tenant:\n",
    "  - **Invocations**: Number of API calls\n",
    "  - **InputTokenCount**: Request complexity\n",
    "  - **OutputTokenCount**: Response generation\n",
    "- Generates time-series plots showing usage patterns\n",
    "- Returns metrics dictionary for cost analysis\n",
    "\n",
    "**Why it matters:**\n",
    "- Each tenant's metrics are **isolated** using their unique AIP ID\n",
    "- Enables accurate per-tenant billing and cost allocation\n",
    "- Provides proof that multi-tenancy isolation is working through LiteLLM\n",
    "\n",
    "**Key insight:** Even though requests go through LiteLLM abstraction layer, CloudWatch still tracks them separately by AIP dimension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_aip_usage(tenant_aips, region):\n",
    "    \"\"\"\n",
    "    Monitor CloudWatch metrics for multiple Application Inference Profiles.\n",
    "    \n",
    "    Args:\n",
    "        tenant_aips (dict): Mapping of tenant_id -> AIP ARN\n",
    "        region (str): AWS region where AIPs are deployed\n",
    "    \n",
    "    Returns:\n",
    "        dict: Tenant metrics for further analysis\n",
    "    \"\"\"\n",
    "    from lab_helpers.cloudwatch import fetch_metrices, plot_graph\n",
    "\n",
    "    print(\"üìä Fetching CloudWatch metrics for Application Inference Profiles...\")\n",
    "    print(f\"Region: {region}\")\n",
    "    print(f\"Time Range: Last 60 minutes\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    tenant_metrics = {}\n",
    "\n",
    "    for tenant_id, aip_arn in tenant_aips.items():\n",
    "        print(f\"\\nüè¢ TENANT: {tenant_id.upper()}\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"AIP ARN: {aip_arn}\")\n",
    "        \n",
    "        # Extract AIP ID from full ARN for CloudWatch ModelId dimension\n",
    "        aip_id = aip_arn.split('/')[-1]\n",
    "        print(f\"AIP ID: {aip_id}\")\n",
    "        \n",
    "        try:\n",
    "            print(f\"\\nüìä METRICS FOR {tenant_id.upper()}:\")\n",
    "            response, input_token_response, output_token_response = fetch_metrices(\n",
    "                Region=region,\n",
    "                Period=60,\n",
    "                Timedelta=60,\n",
    "                Id=aip_id\n",
    "            )\n",
    "            \n",
    "            tenant_metrics[tenant_id] = {\n",
    "                'invocations': response,\n",
    "                'input_tokens': input_token_response,\n",
    "                'output_tokens': output_token_response\n",
    "            }\n",
    "            \n",
    "            print(f\"\\nüìà USAGE PLOTS FOR {tenant_id.upper()}:\")\n",
    "            plot_graph(response, input_token_response, output_token_response)\n",
    "            \n",
    "            print(\"=\"*50)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è CloudWatch error for {tenant_id}: {str(e)}\")\n",
    "            print(f\"üí° Metrics may take a few minutes to appear after model invocations\")\n",
    "            print(\"=\"*50)\n",
    "    \n",
    "    return tenant_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Baseline Check: CloudWatch Metrics BEFORE LiteLLM Requests\n",
    "\n",
    "Before we route any requests through LiteLLM, let's check CloudWatch metrics for our newly created AIPs. This establishes a **baseline** that proves:\n",
    "\n",
    "1. ‚úÖ The AIPs exist and are properly configured\n",
    "2. ‚úÖ Each tenant has their own isolated CloudWatch dimension\n",
    "3. ‚úÖ No usage has occurred yet (metrics should be empty)\n",
    "\n",
    "### The Before/After Demonstration\n",
    "\n",
    "We'll check metrics **twice** in this lab:\n",
    "\n",
    "**üîµ NOW (BEFORE):** Check metrics ‚Üí Expect empty/no data\n",
    "- This proves the AIPs are ready but haven't been used\n",
    "\n",
    "**üü¢ LATER (AFTER):** Check metrics ‚Üí Expect populated per-tenant data  \n",
    "- This proves multi-tenant isolation works through LiteLLM!\n",
    "\n",
    "Let's run the baseline check:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Tenant-Aware Request Routing via LiteLLM Gateway\n",
    "\n",
    "Now let's implement the core gateway functionality: routing tenant requests through LiteLLM to their specific Application Inference Profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ LiteLLM Request Routing Function\n",
    "\n",
    "This function demonstrates the power of LiteLLM gateway abstraction for multi-tenant applications.\n",
    "\n",
    "**The Gateway Pattern:**\n",
    "```python\n",
    "litellm.completion(\n",
    "    model=config[\"model\"],  # Tenant-specific AIP routing\n",
    "    messages=[{\"role\": \"user\", \"content\": message}],\n",
    "    **parameters\n",
    ")\n",
    "```\n",
    "\n",
    "**What happens behind the scenes:**\n",
    "1. LiteLLM receives request with tenant-specific model config\n",
    "2. Extracts AIP ARN from config\n",
    "3. Translates to AWS Bedrock API call format\n",
    "4. Invokes Nova Pro through the tenant's AIP\n",
    "5. Returns response with usage metrics\n",
    "6. CloudWatch automatically tracks under tenant's AIP dimension\n",
    "\n",
    "**Key Benefits:**\n",
    "- **Single code path**: Same function works for all tenants\n",
    "- **Configuration-driven**: Add new tenants by updating config, not code\n",
    "- **Framework-agnostic**: Works with any LiteLLM-supported provider\n",
    "- **Built-in features**: Token counting, retries, error handling included\n",
    "\n",
    "Compare this to Lab 03 where we needed manual routing logic and error handling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_tenant_request_via_litellm(tenant_id: str, config: Dict[str, Any], user_message: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Route tenant request through LiteLLM gateway to their specific AIP.\n",
    "    \n",
    "    Args:\n",
    "        tenant_id: Tenant identifier\n",
    "        config: LiteLLM configuration for this tenant\n",
    "        user_message: User's message to process\n",
    "        \n",
    "    Returns:\n",
    "        Response with content and usage metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Record start time for latency calculation\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # üåâ LITELLM GATEWAY CALL - Single interface for all tenants!\n",
    "        # LiteLLM transparently routes through tenant's AIP\n",
    "        response = litellm.completion(\n",
    "            model=config[\"model\"],  # bedrock/converse/<full-aip-arn>\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            max_tokens=config[\"max_tokens\"],\n",
    "            # Note: temperature removed - not supported with Bedrock AIPs via LiteLLM\n",
    "            aws_region_name=config[\"aws_region_name\"]\n",
    "        )\n",
    "        \n",
    "        # Calculate latency\n",
    "        end_time = datetime.now()\n",
    "        latency_ms = (end_time - start_time).total_seconds() * 1000\n",
    "        \n",
    "        # Extract response content\n",
    "        # LiteLLM normalizes response format across all providers\n",
    "        response_content = response.choices[0].message.content\n",
    "        \n",
    "        # Extract usage metrics\n",
    "        # LiteLLM provides consistent token counting across providers\n",
    "        usage_metrics = {\n",
    "            'input_tokens': response.usage.prompt_tokens,\n",
    "            'output_tokens': response.usage.completion_tokens,\n",
    "            'total_tokens': response.usage.total_tokens,\n",
    "            'latency_ms': latency_ms\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'tenant_id': tenant_id,\n",
    "            'content': response_content,\n",
    "            'usage_metrics': usage_metrics,\n",
    "            'aip_arn': config['metadata']['aip_arn'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'success': True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'tenant_id': tenant_id,\n",
    "            'error': str(e),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'success': False\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ LiteLLM request routing function defined\")\n",
    "print(\"üí° This single function handles ALL tenant requests through gateway!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Define Multi-Tenant Use Cases\n",
    "\n",
    "Let's create realistic use cases for our two tenants to demonstrate how the LiteLLM gateway handles different request patterns while maintaining isolation:\n",
    "\n",
    "- **Tenant A (SaaS Support)**: Customer support chatbot automation\n",
    "- **Tenant B (Analytics)**: Batch report generation\n",
    "\n",
    "Each use case will route through their tenant-specific AIP via LiteLLM, with all usage tracked separately in CloudWatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define use cases for each tenant\n",
    "TENANT_USE_CASES = {\n",
    "    \"tenant_a\": {\n",
    "        \"use_case\": \"Customer Support Chatbot\",\n",
    "        \"message\": \"\"\"\n",
    "A customer just submitted this support ticket:\n",
    "\n",
    "\"I'm trying to integrate your API with our system, but I keep getting a 401 Unauthorized error. \n",
    "I've checked my API key multiple times and it looks correct. The error happens on every endpoint I try.\n",
    "Can you help me understand what might be wrong?\"\n",
    "\n",
    "Please provide a helpful, friendly support response that:\n",
    "1. Acknowledges the issue\n",
    "2. Suggests 3 common causes of 401 errors\n",
    "3. Provides clear troubleshooting steps\n",
    "4. Offers next steps if these don't resolve it\n",
    "\"\"\"\n",
    "    },\n",
    "    \"tenant_b\": {\n",
    "        \"use_case\": \"Analytics Report Generation\",\n",
    "        \"message\": \"\"\"\n",
    "Generate an executive summary for the following analytics data:\n",
    "\n",
    "Monthly User Engagement Metrics:\n",
    "- Total Active Users: 45,230 (‚Üë 12% MoM)\n",
    "- Average Session Duration: 8.5 minutes (‚Üë 5% MoM)\n",
    "- Feature Adoption Rate: 67% (‚Üë 9% MoM)\n",
    "- Customer Satisfaction Score: 4.2/5.0 (‚Üë 0.3 MoM)\n",
    "- Churn Rate: 2.1% (‚Üì 0.4% MoM)\n",
    "\n",
    "Please provide:\n",
    "1. Executive summary of key insights\n",
    "2. Trend analysis\n",
    "3. Strategic recommendations\n",
    "4. Areas of concern or opportunity\n",
    "\"\"\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìã Multi-Tenant Use Cases Defined:\")\n",
    "print(\"=\"*60)\n",
    "for tenant_id, details in TENANT_USE_CASES.items():\n",
    "    print(f\"\\nüè¢ {tenant_id.upper()}: {details['use_case']}\")\n",
    "    print(f\"   Message length: {len(details['message'])} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üöÄ Process Requests Through LiteLLM Gateway\n",
    "\n",
    "Now let's route both tenant requests through the LiteLLM gateway. Watch how:\n",
    "1. Same routing function handles both tenants\n",
    "2. Each request goes through their specific AIP\n",
    "3. Usage metrics are captured per tenant\n",
    "4. CloudWatch will track them separately (we'll verify in Section 3!)\n",
    "\n",
    "**The Gateway Magic:** No conditional logic, no tenant-specific code paths - just configuration-driven routing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Route requests through LiteLLM gateway for both tenants\n",
    "tenant_responses = {}\n",
    "\n",
    "print(\"üöÄ Routing tenant requests through LiteLLM gateway...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for tenant_id in TENANT_CONFIGS.keys():\n",
    "    if tenant_id in litellm_config:\n",
    "        print(f\"\\nüåâ Processing request for {tenant_id.upper()} via LiteLLM...\")\n",
    "        print(f\"   Use Case: {TENANT_USE_CASES[tenant_id]['use_case']}\")\n",
    "        print(f\"   Routing through: {litellm_config[tenant_id]['model']}\")\n",
    "        \n",
    "        result = route_tenant_request_via_litellm(\n",
    "            tenant_id=tenant_id,\n",
    "            config=litellm_config[tenant_id],\n",
    "            user_message=TENANT_USE_CASES[tenant_id]['message']\n",
    "        )\n",
    "        \n",
    "        tenant_responses[tenant_id] = result\n",
    "        \n",
    "        if result['success']:\n",
    "            print(f\"‚úÖ Request completed for {tenant_id}\")\n",
    "            print(f\"   Input tokens: {result['usage_metrics']['input_tokens']}\")\n",
    "            print(f\"   Output tokens: {result['usage_metrics']['output_tokens']}\")\n",
    "            print(f\"   Total tokens: {result['usage_metrics']['total_tokens']}\")\n",
    "            print(f\"   Latency: {result['usage_metrics']['latency_ms']:.2f}ms\")\n",
    "        else:\n",
    "            print(f\"‚ùå Error for {tenant_id}: {result['error']}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(f\"üìä Processed {len([r for r in tenant_responses.values() if r['success']])} successful requests\")\n",
    "print(\"üí° Each request routed through tenant-specific AIP via LiteLLM!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìÑ Display Response Content and Metrics\n",
    "\n",
    "Let's examine the AI-generated responses for each tenant and their corresponding usage metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display responses for each tenant\n",
    "print(\"üìÑ TENANT RESPONSES VIA LITELLM GATEWAY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for tenant_id, response in tenant_responses.items():\n",
    "    if response['success']:\n",
    "        print(f\"\\nüè¢ TENANT: {tenant_id.upper()}\")\n",
    "        print(f\"   Use Case: {TENANT_USE_CASES[tenant_id]['use_case']}\")\n",
    "        print(f\"   AIP ARN: {response['aip_arn']}\")\n",
    "        print(\"   \" + \"-\"*76)\n",
    "        print(f\"\\n   üìù RESPONSE:\\n\")\n",
    "        \n",
    "        # Display response content with indentation\n",
    "        for line in response['content'].split('\\n'):\n",
    "            print(f\"   {line}\")\n",
    "        \n",
    "        print(f\"\\n   \" + \"-\"*76)\n",
    "        print(f\"   üìä USAGE METRICS:\")\n",
    "        print(f\"      Input Tokens:  {response['usage_metrics']['input_tokens']:,}\")\n",
    "        print(f\"      Output Tokens: {response['usage_metrics']['output_tokens']:,}\")\n",
    "        print(f\"      Total Tokens:  {response['usage_metrics']['total_tokens']:,}\")\n",
    "        print(f\"      Latency:       {response['usage_metrics']['latency_ms']:.2f}ms\")\n",
    "        print(f\"      Timestamp:     {response['timestamp']}\")\n",
    "        print(\"=\"*80)\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {tenant_id.upper()}: Failed - {response['error']}\")\n",
    "        print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: CloudWatch Metrics and Multi-Tenant Isolation Proof\n",
    "\n",
    "Now let's verify that despite routing through LiteLLM, CloudWatch still tracks each tenant's usage separately through their Application Inference Profiles!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç How LiteLLM + AIP Maintains CloudWatch Isolation\n",
    "\n",
    "**The Question:** If we're routing through LiteLLM abstraction, how does CloudWatch still track tenants separately?\n",
    "\n",
    "**The Answer:** The magic of Application Inference Profiles!\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Tenant A    ‚îÇ\n",
    "‚îÇ Request     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ LiteLLM Gateway\n",
    "       ‚îÇ            (Unified Interface)\n",
    "       ‚îÇ                   ‚îÇ\n",
    "       ‚îÇ                   ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ bedrock/tenant-a-aip\n",
    "       ‚îÇ                   ‚îÇ        ‚îÇ\n",
    "       ‚îÇ                   ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ AWS Bedrock\n",
    "       ‚îÇ                   ‚îÇ                 (Tenant A AIP ARN)\n",
    "       ‚îÇ                   ‚îÇ                        ‚îÇ\n",
    "       ‚îÇ                   ‚îÇ                        ‚îî‚îÄ‚îÄ‚ñ∫ CloudWatch\n",
    "       ‚îÇ                   ‚îÇ                             Dimension: tenant-a-aip\n",
    "       ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ Tenant B    ‚îÇ\n",
    "‚îÇ Request     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "       ‚îÇ\n",
    "       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ LiteLLM Gateway\n",
    "                    (Same Code!)\n",
    "                           ‚îÇ\n",
    "                           ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ bedrock/tenant-b-aip\n",
    "                           ‚îÇ        ‚îÇ\n",
    "                           ‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ AWS Bedrock\n",
    "                           ‚îÇ                 (Tenant B AIP ARN)\n",
    "                           ‚îÇ                        ‚îÇ\n",
    "                           ‚îÇ                        ‚îî‚îÄ‚îÄ‚ñ∫ CloudWatch\n",
    "                           ‚îÇ                             Dimension: tenant-b-aip\n",
    "```\n",
    "\n",
    "**Key Points:**\n",
    "1. LiteLLM routes to `bedrock/<aip_id>` (tenant-specific)\n",
    "2. Each AIP ID maps to unique AIP ARN\n",
    "3. AWS Bedrock logs metrics with AIP ARN as `ModelId` dimension\n",
    "4. CloudWatch separates metrics by `ModelId` dimension\n",
    "5. **Result:** Per-tenant isolation maintained automatically!\n",
    "\n",
    "**The Proof:** Let's check CloudWatch metrics now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for CloudWatch metrics to propagate\n",
    "print(\"‚è≥ Waiting 60 seconds for CloudWatch metrics to propagate...\")\n",
    "print(\"üí° CloudWatch metrics have a 1-2 minute delay after API calls\")\n",
    "time.sleep(60)\n",
    "print(\"‚úÖ Wait complete - proceeding with metrics monitoring\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üü¢ AFTER Check: CloudWatch Metrics Show Multi-Tenant Isolation!\n",
    "\n",
    "Now that we've routed requests through LiteLLM, let's check CloudWatch metrics. You should see:\n",
    "\n",
    "### Expected Results:\n",
    "\n",
    "**üìä Per-Tenant Metrics:**\n",
    "- **Tenant A** metrics under their own AIP dimension\n",
    "- **Tenant B** metrics under their own AIP dimension\n",
    "- **Separate graphs** for each tenant (not combined!)\n",
    "\n",
    "### The Proof of Multi-Tenancy Through Gateway:\n",
    "\n",
    "Compare this to the BEFORE check:\n",
    "- **BEFORE:** Empty metrics (AIPs ready but unused)\n",
    "- **AFTER:** Populated metrics showing exact per-tenant usage\n",
    "\n",
    "This demonstrates:\n",
    "1. ‚úÖ **Gateway + Isolation**: LiteLLM routing preserves per-tenant tracking\n",
    "2. ‚úÖ **Accurate Billing**: Can charge tenant_a and tenant_b independently\n",
    "3. ‚úÖ **SLA Monitoring**: Can track performance per customer through gateway\n",
    "4. ‚úÖ **Cost Allocation**: Know exactly what each tenant costs despite abstraction layer\n",
    "\n",
    "‚è≥ **Note:** If metrics appear empty, wait another 60 seconds and re-run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CloudWatch metrics AFTER LiteLLM requests\n",
    "print(\"üü¢ AFTER CHECK: Querying CloudWatch for tenant-specific metrics...\")\n",
    "print(\"=\"*80)\n",
    "print(\"Expected: Per-tenant metrics showing isolated usage through LiteLLM!\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "after_metrics = monitor_aip_usage(tenant_aips, Region)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Metrics check complete!\")\n",
    "print(\"üí° If you see metrics data, multi-tenant isolation is working!\")\n",
    "print(\"   Each tenant's LiteLLM requests tracked separately via their AIP.\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üí∞ Cost Allocation Analysis\n",
    "\n",
    "Let's demonstrate how the combination of LiteLLM + AIP enables accurate per-tenant cost allocation and billing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìà Before/After Comparison Summary\n",
    "\n",
    "Let's summarize the before/after demonstration that proves multi-tenant isolation works through LiteLLM gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before/After comparison summary\n",
    "print(\"üìä BEFORE/AFTER COMPARISON: Multi-Tenant Isolation Proof\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîµ BEFORE (Baseline Check):\")\n",
    "print(\"   Status: AIPs created and configured\")\n",
    "print(\"   LiteLLM: Gateway configurations ready\")\n",
    "print(\"   CloudWatch: No metrics (AIPs unused)\")\n",
    "print(\"   Tenant Isolation: Ready but unverified\")\n",
    "\n",
    "print(\"\\nüü¢ AFTER (Post-LiteLLM Requests):\")\n",
    "print(\"   LiteLLM Requests: ‚úÖ Routed through gateway\")\n",
    "print(\"   Tenant Responses: ‚úÖ Generated via AIPs\")\n",
    "print(\"   CloudWatch Metrics: ‚úÖ Separate dimensions per tenant\")\n",
    "print(\"   Cost Tracking: ‚úÖ Per-tenant billing enabled\")\n",
    "\n",
    "print(\"\\nüìä WHAT WE PROVED:\")\n",
    "print(\"   1. ‚úÖ LiteLLM gateway routing preserves AIP isolation\")\n",
    "print(\"   2. ‚úÖ Each tenant's usage tracked separately in CloudWatch\")\n",
    "print(\"   3. ‚úÖ Configuration-driven approach scales to N tenants\")\n",
    "print(\"   4. ‚úÖ Gateway abstraction + AIP = production-ready multi-tenancy\")\n",
    "print(\"   5. ‚úÖ Accurate cost allocation despite abstraction layer\")\n",
    "\n",
    "print(\"\\nüí° THE WINNING COMBINATION:\")\n",
    "print(\"   LiteLLM:    Simplifies multi-tenant routing code\")\n",
    "print(\"   +\")\n",
    "print(\"   AIP:        Ensures CloudWatch tracking isolation\")\n",
    "print(\"   =\")\n",
    "print(\"   Production: Scalable, maintainable, trackable multi-tenant AI\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 05 Summary\n",
    "\n",
    "üéâ **Congratulations!** You've successfully implemented Application Inference Profiles with LiteLLM gateway for a multi-tenant AI platform.\n",
    "\n",
    "### What You Accomplished:\n",
    "\n",
    "1. **‚úÖ Created Application Inference Profiles** for two tenants using boto3\n",
    "2. **‚úÖ Configured LiteLLM gateway** to route through tenant-specific AIPs\n",
    "3. **‚úÖ Checked CloudWatch metrics BEFORE** gateway requests (baseline/empty state)\n",
    "4. **‚úÖ Routed tenant requests through LiteLLM** with unified code interface\n",
    "5. **‚úÖ Generated tenant-specific responses** with isolated usage tracking\n",
    "6. **‚úÖ Checked CloudWatch metrics AFTER** gateway requests (per-tenant usage visible!)\n",
    "7. **‚úÖ Analyzed cost allocation** and billing breakdown per tenant\n",
    "8. **‚úÖ Proved multi-tenant isolation** works through gateway abstraction\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Gateway Abstraction**: LiteLLM simplifies multi-tenant routing with single code path\n",
    "- **Configuration-Driven**: Add new tenants by updating config, not code\n",
    "- **Tenant Isolation**: AIP ensures CloudWatch tracking remains separate\n",
    "- **Cost Allocation**: Built-in token counting enables accurate billing\n",
    "- **Production Ready**: Gateway pattern scales to many tenants efficiently\n",
    "- **Framework Agnostic**: LiteLLM works across AWS, OpenAI, Azure, and more\n",
    "\n",
    "### What You Learned:\n",
    "\n",
    "```python\n",
    "# Traditional approach (Lab 03 - Direct boto3)\n",
    "if tenant_id == \"tenant_a\":\n",
    "    response = bedrock_runtime.invoke_model(modelId=tenant_a_aip_arn, ...)\n",
    "elif tenant_id == \"tenant_b\":\n",
    "    response = bedrock_runtime.invoke_model(modelId=tenant_b_aip_arn, ...)\n",
    "# Adding new tenants requires code changes\n",
    "\n",
    "# Gateway approach (Lab 05 - LiteLLM + AIP)\n",
    "response = litellm.completion(\n",
    "    model=litellm_config[tenant_id][\"model\"],  # Config-driven routing\n",
    "    messages=[{\"role\": \"user\", \"content\": message}]\n",
    ")\n",
    "# Adding new tenants only requires config changes\n",
    "```\n",
    "\n",
    "### The Before/After Pattern:\n",
    "\n",
    "- **BEFORE Check**: Empty CloudWatch metrics (AIPs ready but unused)\n",
    "- **AFTER Check**: Populated metrics showing exact per-tenant usage via LiteLLM\n",
    "- **Proof**: Multi-tenant isolation works through gateway abstraction!\n",
    "\n",
    "### Architecture Benefits:\n",
    "\n",
    "| Aspect | Value |\n",
    "|--------|-------|\n",
    "| **Code Simplicity** | Single routing function for all tenants |\n",
    "| **Scalability** | Config-driven tenant management |\n",
    "| **Observability** | Per-tenant CloudWatch metrics automatic |\n",
    "| **Cost Tracking** | Built-in token counting and billing |\n",
    "| **Multi-Provider** | Easy to add OpenAI, Azure, etc. |\n",
    "| **Maintainability** | Less code to maintain and test |\n",
    "\n",
    "### Lab Comparison:\n",
    "\n",
    "| Lab | Focus | Approach | Best For |\n",
    "|-----|-------|----------|----------|\n",
    "| **Lab 03** | Direct boto3 | Manual routing logic | Full AWS Bedrock control |\n",
    "| **Lab 05** | LiteLLM gateway | Config-driven routing | Multi-tenant scale, multi-provider |\n",
    "| **Lab 06** | LangChain | Chain-based apps | Complex workflows |\n",
    "| **Lab 07** | LangGraph | Graph workflows | Multi-step processes |\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "Continue your AIP journey with framework-specific integrations:\n",
    "- **Lab 06 (Optional)**: LangChain - Chain-based applications with tenant awareness\n",
    "- **Lab 07 (Optional)**: LangGraph - Graph workflows for complex multi-step processes\n",
    "- **Lab 08**: AgentCore Runtime - Production deployment with full observability\n",
    "\n",
    "**Ready to explore chain-based applications?** ‚Üí [Continue to Lab 07: LangChain (Optional)](lab-07-aip-langchain-optional.ipynb)\n",
    "\n",
    "**Already deployed with AgentCore?** ‚Üí [Back to Lab 05: AgentCore Runtime](lab-05-agentcore-runtime-deployment.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
